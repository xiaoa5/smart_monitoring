"""
Path 2 Implementation: Stage 1 & 2
Motion Sequence Generator + LSTM-Based Multi-Object Tracker

Author: AI Assistant
Date: 2025-11-14
Status: Ready for Testing

Dependencies:
    pip install pybullet numpy torch opencv-python matplotlib pyyaml --break-system-packages
"""

import pybullet as p
import pybullet_data
import numpy as np
import cv2
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import math
from dataclasses import dataclass, asdict
from enum import Enum

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


# ============================================================================
# Stage 1: Motion Sequence Generator
# ============================================================================

class MotionType(Enum):
    """æ”¯æŒçš„è¿åŠ¨ç±»å‹"""
    LINEAR = "linear"
    CIRCULAR = "circular"
    RANDOM_WALK = "random_walk"
    STATIONARY = "stationary"


@dataclass
class ObjectState:
    """ç‰©ä½“çŠ¶æ€"""
    id: int
    pos_3d: List[float]  # [x, y, z] in world coordinates
    bbox: List[float]    # [x1, y1, x2, y2] in image coordinates
    occlusion: float     # 0.0 (visible) to 1.0 (fully occluded)
    velocity: List[float]  # [vx, vy, vz]
    motion_type: str


@dataclass
class FrameData:
    """å•å¸§æ•°æ®"""
    frame: int
    timestamp: float
    camera_id: int
    objects: List[ObjectState]


class MotionSequenceGenerator:
    """
    é˜¶æ®µ1: è¿åŠ¨åºåˆ—ç”Ÿæˆå™¨
    ç”Ÿæˆå¯æ§çš„å¤šç›®æ ‡è¿ç»­è½¨è¿¹æ•°æ®
    """
    
    def __init__(
        self,
        scene_size: Tuple[float, float] = (10.0, 10.0),
        num_cameras: int = 4,
        fps: int = 30,
        output_dir: str = "./motion_sequences"
    ):
        self.scene_size = scene_size
        self.num_cameras = num_cameras
        self.fps = fps
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # PyBulletåˆå§‹åŒ–
        self.client = p.connect(p.DIRECT)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -10)
        
        # åœºæ™¯è®¾ç½®
        self.plane_id = p.loadURDF("plane.urdf")
        self.object_ids = []
        self.object_motions = {}
        
        # ç›¸æœºè®¾ç½®
        self.cameras = self._setup_cameras()
        
    def _setup_cameras(self) -> List[Dict]:
        """è®¾ç½®å¤šç›¸æœºé…ç½®"""
        cameras = []
        width, height = 640, 480
        fov = 60
        aspect = width / height
        near, far = 0.1, 100
        
        # å››å‘¨ç›¸æœºå¸ƒå±€
        positions = [
            [self.scene_size[0]/2, -2, 3],   # South
            [self.scene_size[0]/2, self.scene_size[1]+2, 3],  # North
            [-2, self.scene_size[1]/2, 3],   # West
            [self.scene_size[0]+2, self.scene_size[1]/2, 3]   # East
        ]
        
        targets = [[self.scene_size[0]/2, self.scene_size[1]/2, 0]] * 4
        
        for i in range(self.num_cameras):
            view_matrix = p.computeViewMatrix(
                cameraEyePosition=positions[i],
                cameraTargetPosition=targets[i],
                cameraUpVector=[0, 0, 1]
            )
            
            proj_matrix = p.computeProjectionMatrixFOV(
                fov=fov, aspect=aspect, nearVal=near, farVal=far
            )
            
            cameras.append({
                'id': i,
                'position': positions[i],
                'target': targets[i],
                'view_matrix': view_matrix,
                'proj_matrix': proj_matrix,
                'width': width,
                'height': height,
                'fov': fov
            })
            
        return cameras
    
    def add_object(
        self,
        obj_id: int,
        start_pos: List[float],
        motion_type: MotionType,
        **motion_params
    ):
        """æ·»åŠ è¿åŠ¨ç‰©ä½“"""
        # åˆ›å»ºç®€å•çš„ç«‹æ–¹ä½“ç‰©ä½“
        collision_shape = p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.3, 0.3, 0.3])
        visual_shape = p.createVisualShape(
            p.GEOM_BOX,
            halfExtents=[0.3, 0.3, 0.3],
            rgbaColor=[np.random.rand(), np.random.rand(), np.random.rand(), 1]
        )
        
        body_id = p.createMultiBody(
            baseMass=1.0,
            baseCollisionShapeIndex=collision_shape,
            baseVisualShapeIndex=visual_shape,
            basePosition=start_pos
        )
        
        self.object_ids.append(body_id)
        
        # ä¿å­˜è¿åŠ¨å‚æ•°
        self.object_motions[body_id] = {
            'id': obj_id,
            'type': motion_type,
            'start_pos': start_pos,
            'params': motion_params,
            'time': 0.0
        }
        
        return body_id
    
    def _update_object_motion(self, body_id: int, dt: float):
        """æ›´æ–°ç‰©ä½“è¿åŠ¨"""
        motion = self.object_motions[body_id]
        motion['time'] += dt
        
        t = motion['time']
        motion_type = motion['type']
        params = motion['params']
        start_pos = motion['start_pos']
        
        if motion_type == MotionType.LINEAR:
            # ç›´çº¿è¿åŠ¨
            velocity = params.get('velocity', [1.0, 0.0, 0.0])
            new_pos = [
                start_pos[0] + velocity[0] * t,
                start_pos[1] + velocity[1] * t,
                start_pos[2] + velocity[2] * t
            ]
            
        elif motion_type == MotionType.CIRCULAR:
            # åœ†å‘¨è¿åŠ¨
            center = params.get('center', [5.0, 5.0, 0.5])
            radius = params.get('radius', 2.0)
            angular_velocity = params.get('angular_velocity', 1.0)
            
            angle = angular_velocity * t
            new_pos = [
                center[0] + radius * np.cos(angle),
                center[1] + radius * np.sin(angle),
                center[2]
            ]
            
        elif motion_type == MotionType.RANDOM_WALK:
            # éšæœºæ¸¸èµ°
            step_size = params.get('step_size', 0.1)
            current_pos, _ = p.getBasePositionAndOrientation(body_id)
            
            # æ·»åŠ éšæœºæ‰°åŠ¨
            noise = np.random.randn(3) * step_size
            new_pos = [
                np.clip(current_pos[0] + noise[0], 0, self.scene_size[0]),
                np.clip(current_pos[1] + noise[1], 0, self.scene_size[1]),
                current_pos[2]
            ]
            
        else:  # STATIONARY
            new_pos = start_pos
        
        p.resetBasePositionAndOrientation(
            body_id,
            new_pos,
            [0, 0, 0, 1]
        )
        
        return new_pos
    
    def _compute_occlusion(self, obj_pos: List[float], camera: Dict) -> float:
        """è®¡ç®—é®æŒ¡ç¨‹åº¦(ç®€åŒ–ç‰ˆ)"""
        # ä½¿ç”¨æ·±åº¦å›¾è®¡ç®—é®æŒ¡
        # è¿™é‡Œç®€åŒ–å®ç°,å®é™…åº”è¯¥ç”¨raycast
        return np.random.uniform(0, 0.3)  # ç®€åŒ–: éšæœºé®æŒ¡
    
    def _project_to_image(
        self,
        pos_3d: List[float],
        camera: Dict
    ) -> Optional[List[float]]:
        """3Dç‚¹æŠ•å½±åˆ°å›¾åƒåæ ‡"""
        # ç®€åŒ–çš„æŠ•å½±,å®é™…åº”è¯¥ç”¨å®Œæ•´çš„ç›¸æœºçŸ©é˜µ
        cam_pos = camera['position']
        width, height = camera['width'], camera['height']
        
        # è®¡ç®—ç›¸å¯¹ä½ç½®
        dx = pos_3d[0] - cam_pos[0]
        dy = pos_3d[1] - cam_pos[1]
        dz = pos_3d[2] - cam_pos[2]
        
        dist = np.sqrt(dx**2 + dy**2 + dz**2)
        if dist < 0.1:
            return None
        
        # ç®€åŒ–çš„é€è§†æŠ•å½±
        fov = camera['fov']
        scale = height / (2 * np.tan(np.radians(fov) / 2))
        
        x = width/2 + (dx / dist) * scale
        y = height/2 - (dz / dist) * scale
        
        # è¾¹ç•Œæ¡†(å‡è®¾å›ºå®šå¤§å°)
        box_size = 50 / dist  # è¿‘å¤§è¿œå°
        
        bbox = [
            max(0, x - box_size),
            max(0, y - box_size),
            min(width, x + box_size),
            min(height, y + box_size)
        ]
        
        # æ£€æŸ¥æ˜¯å¦åœ¨è§†é‡å†…
        if bbox[2] > 0 and bbox[0] < width and bbox[3] > 0 and bbox[1] < height:
            return bbox
        return None
    
    def generate_sequence(
        self,
        duration: float = 10.0,
        save_video: bool = True,
        save_json: bool = True
    ) -> List[FrameData]:
        """ç”Ÿæˆè¿åŠ¨åºåˆ—"""
        dt = 1.0 / self.fps
        num_frames = int(duration * self.fps)
        
        all_frame_data = []
        
        for frame_idx in range(num_frames):
            t = frame_idx * dt
            
            # æ›´æ–°ç‰©ä½“è¿åŠ¨
            for body_id in self.object_ids:
                self._update_object_motion(body_id, dt)
            
            p.stepSimulation()
            
            # å¯¹æ¯ä¸ªç›¸æœºç”Ÿæˆæ•°æ®
            for camera in self.cameras:
                objects_in_frame = []
                
                for body_id in self.object_ids:
                    pos_3d, _ = p.getBasePositionAndOrientation(body_id)
                    
                    # æŠ•å½±åˆ°å›¾åƒ
                    bbox = self._project_to_image(list(pos_3d), camera)
                    if bbox is None:
                        continue
                    
                    # è®¡ç®—é®æŒ¡
                    occlusion = self._compute_occlusion(pos_3d, camera)
                    
                    # è®¡ç®—é€Ÿåº¦(ç®€åŒ–)
                    motion = self.object_motions[body_id]
                    if motion['type'] == MotionType.LINEAR:
                        velocity = motion['params'].get('velocity', [0, 0, 0])
                    else:
                        velocity = [0, 0, 0]  # ç®€åŒ–
                    
                    obj_state = ObjectState(
                        id=motion['id'],
                        pos_3d=list(pos_3d),
                        bbox=bbox,
                        occlusion=occlusion,
                        velocity=velocity,
                        motion_type=motion['type'].value
                    )
                    
                    objects_in_frame.append(obj_state)
                
                frame_data = FrameData(
                    frame=frame_idx,
                    timestamp=t,
                    camera_id=camera['id'],
                    objects=objects_in_frame
                )
                
                all_frame_data.append(frame_data)
        
        # ä¿å­˜æ•°æ®
        if save_json:
            self._save_json(all_frame_data)
        
        if save_video:
            self._save_video(all_frame_data)
        
        return all_frame_data
    
    def _save_json(self, frame_data: List[FrameData]):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        output_file = self.output_dir / "motion_sequence.json"
        
        data_list = []
        for fd in frame_data:
            frame_dict = {
                'frame': fd.frame,
                'timestamp': fd.timestamp,
                'camera_id': fd.camera_id,
                'objects': [asdict(obj) for obj in fd.objects]
            }
            data_list.append(frame_dict)
        
        with open(output_file, 'w') as f:
            json.dump(data_list, f, indent=2)
        
        print(f"âœ… Saved motion sequence to {output_file}")
    
    def _save_video(self, frame_data: List[FrameData]):
        """ä¿å­˜å¯è§†åŒ–è§†é¢‘(ç®€åŒ–ç‰ˆ)"""
        print("ğŸ“¹ Video generation skipped in this version (requires rendering)")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        p.disconnect(self.client)


# ============================================================================
# Stage 2: LSTM-Based Multi-Object Tracker
# ============================================================================

class TrackingDataset(Dataset):
    """LSTMè·Ÿè¸ªæ•°æ®é›†"""
    
    def __init__(
        self,
        json_file: str,
        sequence_length: int = 10,
        prediction_horizon: int = 5
    ):
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        
        # åŠ è½½æ•°æ®
        with open(json_file, 'r') as f:
            self.data = json.load(f)
        
        # æŒ‰ç›¸æœºå’Œç‰©ä½“IDç»„ç»‡è½¨è¿¹
        self.trajectories = self._organize_trajectories()
        
        # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        self.samples = self._generate_samples()
    
    def _organize_trajectories(self) -> Dict:
        """ç»„ç»‡è½¨è¿¹æ•°æ®"""
        trajectories = {}
        
        for frame_data in self.data:
            cam_id = frame_data['camera_id']
            
            if cam_id not in trajectories:
                trajectories[cam_id] = {}
            
            for obj in frame_data['objects']:
                obj_id = obj['id']
                
                if obj_id not in trajectories[cam_id]:
                    trajectories[cam_id][obj_id] = []
                
                # æå–bboxå’Œä½ç½®ä¿¡æ¯
                trajectories[cam_id][obj_id].append({
                    'frame': frame_data['frame'],
                    'bbox': obj['bbox'],
                    'pos_3d': obj['pos_3d'],
                    'velocity': obj['velocity']
                })
        
        return trajectories
    
    def _generate_samples(self) -> List[Dict]:
        """ç”Ÿæˆè®­ç»ƒæ ·æœ¬"""
        samples = []
        
        for cam_id, objects in self.trajectories.items():
            for obj_id, trajectory in objects.items():
                # ç¡®ä¿è½¨è¿¹è¶³å¤Ÿé•¿
                if len(trajectory) < self.sequence_length + self.prediction_horizon:
                    continue
                
                # æ»‘åŠ¨çª—å£
                for i in range(len(trajectory) - self.sequence_length - self.prediction_horizon + 1):
                    input_seq = trajectory[i:i + self.sequence_length]
                    target_seq = trajectory[i + self.sequence_length:
                                          i + self.sequence_length + self.prediction_horizon]
                    
                    # æå–bboxåºåˆ— [x1, y1, x2, y2]
                    input_bboxes = np.array([t['bbox'] for t in input_seq])
                    target_bboxes = np.array([t['bbox'] for t in target_seq])
                    
                    samples.append({
                        'input': input_bboxes,
                        'target': target_bboxes,
                        'obj_id': obj_id,
                        'cam_id': cam_id
                    })
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        input_seq = torch.FloatTensor(sample['input'])
        target_seq = torch.FloatTensor(sample['target'])
        
        return input_seq, target_seq


class LSTMTracker(nn.Module):
    """
    é˜¶æ®µ2: LSTMå¤šç›®æ ‡è·Ÿè¸ªå™¨
    é¢„æµ‹æœªæ¥çš„è¾¹ç•Œæ¡†æˆ–ä¸–ç•Œåæ ‡ä½ç½®
    """
    
    def __init__(
        self,
        input_size: int = 4,      # bbox: [x1, y1, x2, y2]
        hidden_size: int = 128,
        num_layers: int = 2,
        output_size: int = 4,
        dropout: float = 0.2
    ):
        super(LSTMTracker, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )
    
    def forward(self, x, hidden=None):
        """
        Args:
            x: (batch, seq_len, input_size)
            hidden: (h0, c0) or None
        Returns:
            output: (batch, output_size)
            hidden: (h, c)
        """
        # LSTM forward
        lstm_out, hidden = self.lstm(x, hidden)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_output = lstm_out[:, -1, :]
        
        # å…¨è¿æ¥é¢„æµ‹
        prediction = self.fc(last_output)
        
        return prediction, hidden
    
    def predict_sequence(self, x, steps: int = 5):
        """
        è‡ªå›å½’é¢„æµ‹å¤šæ­¥
        
        Args:
            x: (batch, seq_len, input_size)
            steps: é¢„æµ‹æ­¥æ•°
        Returns:
            predictions: (batch, steps, output_size)
        """
        predictions = []
        current_seq = x.clone()
        hidden = None
        
        for _ in range(steps):
            # é¢„æµ‹ä¸‹ä¸€æ­¥
            pred, hidden = self.forward(current_seq, hidden)
            predictions.append(pred)
            
            # æ›´æ–°åºåˆ—(æ»‘åŠ¨çª—å£)
            current_seq = torch.cat([current_seq[:, 1:, :], pred.unsqueeze(1)], dim=1)
        
        return torch.stack(predictions, dim=1)


class LSTMTrackerTrainer:
    """LSTMè·Ÿè¸ªå™¨è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: LSTMTracker,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
        learning_rate: float = 0.001
    ):
        self.model = model.to(device)
        self.device = device
        
        self.criterion = nn.MSELoss()
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self, train_loader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        for inputs, targets in train_loader:
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            # é¢„æµ‹æ‰€æœ‰æœªæ¥æ­¥
            predictions = self.model.predict_sequence(inputs, steps=targets.size(1))
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predictions, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        return avg_loss
    
    def validate(self, val_loader: DataLoader) -> float:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                predictions = self.model.predict_sequence(inputs, steps=targets.size(1))
                loss = self.criterion(predictions, targets)
                
                total_loss += loss.item()
        
        avg_loss = total_loss / len(val_loader)
        return avg_loss
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int = 50,
        save_path: Optional[str] = None
    ):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        
        for epoch in range(num_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            print(f"Epoch {epoch+1}/{num_epochs} - "
                  f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_path and val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), save_path)
                print(f"  âœ… Saved best model (val_loss: {val_loss:.6f})")
        
        print(f"\nğŸ‰ Training completed! Best val loss: {best_val_loss:.6f}")


# ============================================================================
# Integration & Demo
# ============================================================================

def run_stage1_demo():
    """é˜¶æ®µ1æ¼”ç¤º: ç”Ÿæˆè¿åŠ¨åºåˆ—"""
    print("=" * 60)
    print("Stage 1: Motion Sequence Generator Demo")
    print("=" * 60)
    
    generator = MotionSequenceGenerator(
        scene_size=(10.0, 10.0),
        num_cameras=4,
        fps=30,
        output_dir="./path2_output/stage1"
    )
    
    # æ·»åŠ ä¸åŒè¿åŠ¨æ¨¡å¼çš„ç‰©ä½“
    print("\nğŸ“¦ Adding objects with different motion patterns...")
    
    # 1. ç›´çº¿è¿åŠ¨
    generator.add_object(
        obj_id=1,
        start_pos=[1.0, 1.0, 0.5],
        motion_type=MotionType.LINEAR,
        velocity=[0.5, 0.3, 0.0]
    )
    
    # 2. åœ†å‘¨è¿åŠ¨
    generator.add_object(
        obj_id=2,
        start_pos=[7.0, 5.0, 0.5],
        motion_type=MotionType.CIRCULAR,
        center=[5.0, 5.0, 0.5],
        radius=2.0,
        angular_velocity=0.5
    )
    
    # 3. éšæœºæ¸¸èµ°
    generator.add_object(
        obj_id=3,
        start_pos=[3.0, 7.0, 0.5],
        motion_type=MotionType.RANDOM_WALK,
        step_size=0.05
    )
    
    # ç”Ÿæˆåºåˆ—
    print("\nğŸ¬ Generating motion sequences...")
    frame_data = generator.generate_sequence(
        duration=10.0,
        save_video=False,
        save_json=True
    )
    
    print(f"\nâœ… Generated {len(frame_data)} frames of data")
    print(f"   Saved to: ./path2_output/stage1/motion_sequence.json")
    
    generator.cleanup()
    
    return "./path2_output/stage1/motion_sequence.json"


def run_stage2_demo(json_file: str):
    """é˜¶æ®µ2æ¼”ç¤º: è®­ç»ƒLSTMè·Ÿè¸ªå™¨"""
    print("\n" + "=" * 60)
    print("Stage 2: LSTM-Based Multi-Object Tracker Demo")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“Š Creating dataset...")
    dataset = TrackingDataset(
        json_file=json_file,
        sequence_length=10,
        prediction_horizon=5
    )
    
    print(f"   Total samples: {len(dataset)}")
    
    if len(dataset) == 0:
        print("âš ï¸  No training samples generated. Need longer sequences.")
        return
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    print(f"   Train samples: {train_size}, Val samples: {val_size}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  Creating LSTM Tracker model...")
    model = LSTMTracker(
        input_size=4,
        hidden_size=128,
        num_layers=2,
        output_size=4,
        dropout=0.2
    )
    
    print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒ
    print("\nğŸš€ Starting training...")
    trainer = LSTMTrackerTrainer(
        model=model,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        learning_rate=0.001
    )
    
    os.makedirs("./path2_output/stage2", exist_ok=True)
    
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=30,
        save_path="./path2_output/stage2/best_lstm_tracker.pth"
    )
    
    print("\nâœ… Stage 2 completed!")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ğŸš€ Path 2 Implementation: Stage 1 & 2")
    print("=" * 60)
    
    # è¿è¡Œé˜¶æ®µ1
    json_file = run_stage1_demo()
    
    # è¿è¡Œé˜¶æ®µ2
    run_stage2_demo(json_file)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ All stages completed successfully!")
    print("=" * 60)
    print("\nğŸ“ Output files:")
    print("   Stage 1: ./path2_output/stage1/motion_sequence.json")
    print("   Stage 2: ./path2_output/stage2/best_lstm_tracker.pth")
    print("\nğŸ’¡ Next steps:")
    print("   1. Visualize the generated motion sequences")
    print("   2. Test the LSTM tracker on new sequences")
    print("   3. Integrate with YOLO detection pipeline")
    print("   4. Add ReID for identity consistency (Stage 3)")


if __name__ == "__main__":
    main()
